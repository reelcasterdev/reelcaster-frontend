#!/usr/bin/env tsx

/**
 * Production DFO Regulations Scraper
 * Scrapes fishing regulations from DFO Pacific Region and stores in Supabase
 * Uses hybrid approach: Cheerio (fast) + OpenAI (fallback/validation)
 */

import { config } from 'dotenv'
import path from 'path'
import { createClient } from '@supabase/supabase-js'
import { parseArea, detectChanges, type AreaRegulations } from '../src/app/utils/dfoParser'

// Load environment variables
config({ path: path.join(process.cwd(), '.env.local') })

interface ScrapeResult {
  success: boolean
  area: string
  speciesCount?: number
  changesDetected?: number
  scrapedId?: string
  error?: string
  method?: 'cheerio' | 'openai' | 'hybrid'
}

const AREAS = [
  {
    id: '19',
    name: 'Victoria, Sidney',
    url: 'https://www.pac.dfo-mpo.gc.ca/fm-gp/rec/tidal-maree/a-s19-eng.html',
  },
  {
    id: '20',
    name: 'Sooke',
    url: 'https://www.pac.dfo-mpo.gc.ca/fm-gp/rec/tidal-maree/a-s20-eng.html',
  },
]

/**
 * Initialize Supabase client
 */
function getSupabaseClient() {
  const supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL
  const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY

  if (!supabaseUrl || !supabaseKey) {
    throw new Error('Supabase configuration missing. Add NEXT_PUBLIC_SUPABASE_URL and SUPABASE_SERVICE_ROLE_KEY to .env.local')
  }

  return createClient(supabaseUrl, supabaseKey)
}

/**
 * Scrape a single DFO area and store in Supabase
 */
async function scrapeArea(areaConfig: typeof AREAS[0]): Promise<ScrapeResult> {
  console.log(`\nðŸ“¡ Scraping Area ${areaConfig.id} - ${areaConfig.name}`)
  console.log(`ðŸŒ URL: ${areaConfig.url}`)

  try {
    const supabase = getSupabaseClient()

    // Fetch HTML
    const response = await fetch(areaConfig.url, {
      headers: {
        'User-Agent': 'ReelCaster/1.0 (Fishing Regulation Monitor)',
      },
    })

    if (!response.ok) {
      throw new Error(`HTTP ${response.status}`)
    }

    const html = await response.text()
    console.log(`âœ… Fetched ${(html.length / 1024).toFixed(1)}KB`)

    // Parse with hybrid approach
    console.log('ðŸ”§ Parsing regulations...')
    const scrapedData = await parseArea(html, areaConfig.id, areaConfig.name, areaConfig.url, {
      validateWithAI: true,
    })

    console.log(`âœ… Extracted ${scrapedData.species.length} species`)

    // Get current regulations for comparison
    const { data: currentReg } = await supabase
      .from('fishing_regulations')
      .select('*, species:species_regulations(*)')
      .eq('area_id', areaConfig.id)
      .eq('is_active', true)
      .single()

    // Detect changes
    const changes = detectChanges(currentReg, scrapedData)
    console.log(`ðŸ“Š Changes detected: ${changes.length}`)

    if (changes.length > 0) {
      console.log('ðŸ“ Changes:')
      changes.forEach(change => console.log(`   â€¢ ${change}`))
    }

    // Store in scraped_regulations table
    const { data: scraped, error: insertError } = await supabase
      .from('scraped_regulations')
      .insert({
        area_id: areaConfig.id,
        scraped_data: scrapedData,
        changes_detected: changes,
        approval_status: changes.length > 0 ? 'pending' : 'approved',
      })
      .select()
      .single()

    if (insertError) {
      throw new Error(`Failed to insert into database: ${insertError.message}`)
    }

    console.log(`ðŸ’¾ Stored in Supabase (status: ${scraped.approval_status})`)

    // If no changes and auto-approve enabled, activate immediately
    if (changes.length === 0 && process.env.AUTO_APPROVE_NO_CHANGES === 'true') {
      console.log('âœ… Auto-approving (no changes detected)...')
      // Note: The approval logic is handled by the API endpoint
      // This is just for tracking purposes
    }

    return {
      success: true,
      area: areaConfig.name,
      speciesCount: scrapedData.species.length,
      changesDetected: changes.length,
      scrapedId: scraped.id,
      method: 'hybrid',
    }

  } catch (error) {
    console.error(`âŒ Failed to scrape Area ${areaConfig.id}:`, error)

    // Try to log error to database
    try {
      const supabase = getSupabaseClient()
      await supabase.from('scraped_regulations').insert({
        area_id: areaConfig.id,
        scraped_data: {},
        error_message: error instanceof Error ? error.message : 'Unknown error',
        approval_status: 'rejected',
      })
    } catch (dbError) {
      console.error('Failed to log error to database:', dbError)
    }

    return {
      success: false,
      area: areaConfig.name,
      error: error instanceof Error ? error.message : 'Unknown error',
    }
  }
}

/**
 * Main scraper function
 */
async function main() {
  console.log('ðŸŽ£ DFO Regulations Scraper - Production (Supabase Storage)')
  console.log('='.repeat(70))
  console.log(`ðŸ“… Started: ${new Date().toISOString()}`)
  console.log('')

  // Verify required environment variables
  if (!process.env.NEXT_PUBLIC_SUPABASE_URL || !process.env.SUPABASE_SERVICE_ROLE_KEY) {
    console.error('âŒ Missing required environment variables:')
    console.error('   - NEXT_PUBLIC_SUPABASE_URL')
    console.error('   - SUPABASE_SERVICE_ROLE_KEY')
    console.error('\nPlease add them to .env.local')
    process.exit(1)
  }

  if (!process.env.OPENAI_API_KEY) {
    console.warn('âš ï¸  OPENAI_API_KEY not found. Will use Cheerio only (no AI validation/fallback).')
  }

  const results: ScrapeResult[] = []
  const delay = (ms: number) => new Promise(resolve => setTimeout(resolve, ms))

  // Scrape all areas
  for (const area of AREAS) {
    const result = await scrapeArea(area)
    results.push(result)

    // Delay between areas
    if (area !== AREAS[AREAS.length - 1]) {
      console.log('â¸ï¸  Waiting 3 seconds before next area...')
      await delay(3000)
    }
  }

  // Summary
  console.log('\n' + '='.repeat(70))
  console.log('ðŸ“Š SCRAPING SUMMARY')
  console.log('='.repeat(70))

  const successful = results.filter(r => r.success)
  const failed = results.filter(r => !r.success)

  console.log(`âœ… Successful: ${successful.length}/${results.length}`)
  console.log(`âŒ Failed: ${failed.length}/${results.length}`)

  if (successful.length > 0) {
    console.log('\nâœ… Successfully scraped and stored in Supabase:')
    successful.forEach(r => {
      const changesText = r.changesDetected === 0
        ? '(no changes)'
        : `(${r.changesDetected} changes - pending review)`
      console.log(`   â€¢ ${r.area}: ${r.speciesCount} species ${changesText}`)
    })
  }

  if (failed.length > 0) {
    console.log('\nâŒ Failed areas:')
    failed.forEach(r => {
      console.log(`   â€¢ ${r.area}: ${r.error}`)
    })
  }

  console.log(`\nðŸ’¾ Data stored in: Supabase â†’ scraped_regulations table`)
  console.log(`ðŸ” Review at: ${process.env.NEXT_PUBLIC_APP_URL || 'http://localhost:3004'}/admin/regulations`)
  console.log(`ðŸ“… Completed: ${new Date().toISOString()}`)
  console.log('')

  // Exit with error code if any failed
  if (failed.length > 0) {
    process.exit(1)
  }
}

// Run the scraper
main().catch(error => {
  console.error('ðŸ’¥ Fatal error:', error)
  process.exit(1)
})
